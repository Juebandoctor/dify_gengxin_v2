"""
Dify æ–‡æ¡£åŒæ­¥å·¥å…·
ç”¨äºåŒæ­¥åˆ é™¤æœ¬åœ°æ—¥å¿—ä¸­å·²åœ¨ Dify åˆ é™¤çš„æ–‡æ¡£è®°å½•
"""
import sys
import os
import requests

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

try:
    from utils.config_loader import load_config
    from utils.upload_logger import UploadLogger
    from utils.metadata_manager import MetadataManager
    from utils.logger import log_info, log_success, log_warning, log_error, print_header
except ImportError:
    print("âŒ è¯·å…ˆå®‰è£…ä¾èµ–: pip install pyyaml requests")
    sys.exit(1)


def get_dify_documents(config):
    """ä» Dify è·å–æ‰€æœ‰æ–‡æ¡£ä¿¡æ¯ï¼ˆID å’Œåç§°ï¼‰"""
    base_url = config['dify']['base_url']
    dataset_id = config['dify']['dataset_id']
    api_key = config['dify']['api_key']
    
    headers = {
        'Authorization': f'Bearer {api_key}'
    }
    
    url = f"{base_url}/v1/datasets/{dataset_id}/documents"
    
    try:
        log_info("æ­£åœ¨ä» Dify è·å–æ–‡æ¡£åˆ—è¡¨...")
        
        all_documents = []
        page = 1
        limit = 100
        
        while True:
            params = {
                'page': page,
                'limit': limit
            }
            
            response = requests.get(url, headers=headers, params=params, timeout=30)
            
            if response.status_code != 200:
                log_error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {response.status_code}")
                log_error(f"å“åº”: {response.text[:200]}")
                return None
            
            data = response.json()
            documents = data.get('data', [])
            
            if not documents:
                break
            
            for doc in documents:
                doc_id = doc.get('id')
                doc_name = doc.get('name', '')
                if doc_id:
                    all_documents.append({
                        'id': doc_id,
                        'name': doc_name
                    })
            
            log_info(f"  ç¬¬ {page} é¡µ: {len(documents)} ä¸ªæ–‡æ¡£")
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µ
            if len(documents) < limit:
                break
            
            page += 1
        
        log_success(f"æˆåŠŸè·å– {len(all_documents)} ä¸ªæ–‡æ¡£")
        return all_documents
    
    except Exception as e:
        log_error(f"è·å–æ–‡æ¡£åˆ—è¡¨å‡ºé”™: {e}")
        return None


def sync_metadata(config_path="config.yaml", dry_run=False):
    """
    åŒæ­¥å…ƒæ•°æ®
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        dry_run: æ˜¯å¦ä»…æ¨¡æ‹Ÿè¿è¡Œï¼ˆä¸å®é™…åˆ é™¤ï¼‰
    """
    print_header("Dify æ–‡æ¡£åŒæ­¥å·¥å…·")
    
    # åŠ è½½é…ç½®
    try:
        config = load_config(config_path)
        log_success("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    except Exception as e:
        log_error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
        return False
    
    # åˆå§‹åŒ–æ—¥å¿—ç®¡ç†å™¨
    db_path = config.get('database', {}).get('sqlite_path', './upload_log.db')
    upload_logger = UploadLogger(db_path)
    
    # åˆå§‹åŒ–å…ƒæ•°æ®ç®¡ç†å™¨
    metadata_config = config.get('metadata', {})
    csv_path = metadata_config.get('csv_path', './metadata/source_table.csv')
    metadata_manager = MetadataManager(
        csv_path=csv_path,
        auto_create=metadata_config.get('auto_create', True),
        default_meta=metadata_config.get('default', {})
    )
    
    # è·å–æœ¬åœ°è®°å½•çš„æ–‡æ¡£ ID
    local_doc_ids = upload_logger.get_all_dify_doc_ids()
    log_info(f"æœ¬åœ°æ•°æ®åº“ä¸­æœ‰ {len(local_doc_ids)} æ¡ä¸Šä¼ è®°å½•")
    
    # è·å–æœ¬åœ°å…ƒæ•°æ®è¡¨ä¸­çš„è®°å½•
    local_metadata_titles = metadata_manager.get_all_titles()
    log_info(f"æœ¬åœ°å…ƒæ•°æ®è¡¨ä¸­æœ‰ {len(local_metadata_titles)} æ¡è®°å½•")
    
    # è·å– Dify ä¸­çš„æ–‡æ¡£
    dify_documents = get_dify_documents(config)
    
    if dify_documents is None:
        log_error("æ— æ³•è·å– Dify æ–‡æ¡£åˆ—è¡¨ï¼ŒåŒæ­¥ç»ˆæ­¢")
        return False
    
    log_info(f"Dify çŸ¥è¯†åº“ä¸­æœ‰ {len(dify_documents)} ä¸ªæ–‡æ¡£")
    
    # æå– Dify æ–‡æ¡£ ID å’Œæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    dify_doc_ids = [doc['id'] for doc in dify_documents]
    dify_doc_names = set()
    for doc in dify_documents:
        name = doc['name']
        # ç§»é™¤æ‰©å±•åï¼ˆåŒ…æ‹¬ _ocr.mdï¼‰
        if name.endswith('_ocr.md'):
            name = name[:-7]  # ç§»é™¤ _ocr.md
        elif '.' in name:
            name = os.path.splitext(name)[0]  # ç§»é™¤æ™®é€šæ‰©å±•å
        dify_doc_names.add(name)
    
    # æ‰¾å‡ºéœ€è¦ä»æ•°æ®åº“åˆ é™¤çš„è®°å½•ï¼ˆé€šè¿‡æ–‡æ¡£ IDï¼‰
    db_to_delete = set(local_doc_ids) - set(dify_doc_ids)
    
    # æ‰¾å‡ºéœ€è¦ä»å…ƒæ•°æ®è¡¨åˆ é™¤çš„è®°å½•ï¼ˆé€šè¿‡æ–‡ä»¶åï¼‰
    csv_to_delete = []
    for title in local_metadata_titles:
        # å°è¯•å¤šç§åŒ¹é…æ–¹å¼
        normalized_title = title.replace('ã€Š', '').replace('ã€‹', '').replace('ï¼ˆ', '').replace('ï¼‰', '').strip()
        
        # æ£€æŸ¥æ˜¯å¦åœ¨ Dify ä¸­å­˜åœ¨
        found = False
        for dify_name in dify_doc_names:
            normalized_dify = dify_name.replace('ã€Š', '').replace('ã€‹', '').replace('ï¼ˆ', '').replace('ï¼‰', '').strip()
            if normalized_title == normalized_dify or normalized_title in normalized_dify or normalized_dify in normalized_title:
                found = True
                break
        
        if not found:
            csv_to_delete.append(title)
    
    # æ˜¾ç¤ºåŒæ­¥ç»“æœ
    if not db_to_delete and not csv_to_delete:
        log_success("âœ… æœ¬åœ°è®°å½•ä¸ Dify å®Œå…¨åŒæ­¥ï¼Œæ— éœ€æ¸…ç†")
        return True
    
    print("\n" + "="*50)
    if db_to_delete:
        log_warning(f"æ•°æ®åº“ï¼šå‘ç° {len(db_to_delete)} æ¡éœ€è¦æ¸…ç†çš„è®°å½•")
        print("\nå¾…åˆ é™¤çš„æ•°æ®åº“è®°å½•ï¼ˆæ–‡æ¡£ IDï¼‰ï¼š")
        for i, doc_id in enumerate(list(db_to_delete)[:10], 1):
            print(f"  {i}. {doc_id}")
        if len(db_to_delete) > 10:
            print(f"  ... ä»¥åŠå…¶ä»– {len(db_to_delete) - 10} æ¡è®°å½•")
    
    if csv_to_delete:
        log_warning(f"å…ƒæ•°æ®è¡¨ï¼šå‘ç° {len(csv_to_delete)} æ¡éœ€è¦æ¸…ç†çš„è®°å½•")
        print("\nå¾…åˆ é™¤çš„å…ƒæ•°æ®è®°å½•ï¼ˆæ ‡é¢˜ï¼‰ï¼š")
        for i, title in enumerate(csv_to_delete[:10], 1):
            print(f"  {i}. {title}")
        if len(csv_to_delete) > 10:
            print(f"  ... ä»¥åŠå…¶ä»– {len(csv_to_delete) - 10} æ¡è®°å½•")
    
    print("="*50 + "\n")
    
    if dry_run:
        log_warning("âš ï¸ è¿™æ˜¯æ¨¡æ‹Ÿè¿è¡Œï¼Œä¸ä¼šå®é™…åˆ é™¤è®°å½•")
        return True
    
    # ç¡®è®¤åˆ é™¤
    print("æ˜¯å¦ç»§ç»­åˆ é™¤è¿™äº›è®°å½•ï¼Ÿ[y/N]: ", end='')
    confirm = input().strip().lower()
    
    if confirm != 'y':
        log_info("æ“ä½œå·²å–æ¶ˆ")
        return False
    
    # æ‰§è¡ŒåŒæ­¥åˆ é™¤
    total_deleted = 0
    
    # 1. åˆ é™¤æ•°æ®åº“è®°å½•
    if db_to_delete:
        db_deleted = upload_logger.sync_with_dify(dify_doc_ids)
        if db_deleted > 0:
            log_success(f"âœ… æ•°æ®åº“ï¼šæˆåŠŸåˆ é™¤ {db_deleted} æ¡è®°å½•")
            total_deleted += db_deleted
    
    # 2. åˆ é™¤å…ƒæ•°æ®è¡¨è®°å½•
    if csv_to_delete:
        csv_deleted = metadata_manager.delete_by_titles(csv_to_delete)
        if csv_deleted > 0:
            log_success(f"âœ… å…ƒæ•°æ®è¡¨ï¼šæˆåŠŸåˆ é™¤ {csv_deleted} æ¡è®°å½•")
            total_deleted += csv_deleted
    
    if total_deleted > 0:
        log_success(f"\nğŸ‰ åŒæ­¥å®Œæˆï¼æ€»å…±åˆ é™¤ {total_deleted} æ¡è®°å½•")
    else:
        log_warning("æœªåˆ é™¤ä»»ä½•è®°å½•")
    
    # æ˜¾ç¤ºåŒæ­¥åç»Ÿè®¡
    stats = upload_logger.get_statistics()
    print("\nåŒæ­¥åç»Ÿè®¡ï¼š")
    log_info(f"  æˆåŠŸä¸Šä¼ : {stats['total_success']} ä¸ªæ–‡ä»¶")
    log_info(f"  å¤±è´¥è®°å½•: {stats['total_failed']} ä¸ª")
    log_info(f"  æ€»å¤§å°: {stats['total_size_mb']} MB")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Dify æ–‡æ¡£åŒæ­¥å·¥å…·')
    parser.add_argument('--config', default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…åˆ é™¤')
    
    args = parser.parse_args()
    
    try:
        success = sync_metadata(args.config, dry_run=args.dry_run)
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        log_warning("\næ“ä½œå·²å–æ¶ˆ")
        sys.exit(1)
    except Exception as e:
        log_error(f"åŒæ­¥è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
